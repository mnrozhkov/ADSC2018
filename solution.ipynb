{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import zipfile\n",
    "from tqdm import tqdm_notebook\n",
    "from glob import glob\n",
    "from tqdm import tqdm \n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import scipy\n",
    "from scipy import interp, stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals.joblib import parallel_backend\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer, PolynomialFeatures\n",
    "from sklearn.metrics import roc_curve, auc, average_precision_score, precision_recall_curve, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.externals.joblib import Memory\n",
    "from typing import List, Tuple, Text, TypeVar, Dict, Any\n",
    "import logging\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from tsfresh import extract_features\n",
    "from tsfresh import select_features\n",
    "from tsfresh import extract_relevant_features\n",
    "from tsfresh.feature_extraction.feature_calculators import absolute_sum_of_changes, binned_entropy\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "\n",
    "\n",
    "cachedir = '../cache'\n",
    "memory = Memory(cachedir=cachedir, verbose=3)\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('ytrain.csv')\n",
    "train['train'] = 1\n",
    "\n",
    "test = pd.read_csv('SampleSubmission.csv')\n",
    "test['train'] = 0\n",
    "\n",
    "labels = pd.concat([train, test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels[labels.Id == 219]\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline features extraction (score: .88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def calc_features(df):\n",
    "    max_df = df.max()\n",
    "    min_df = df.min()\n",
    "    mean_df = df.mean()\n",
    "    std_df = df.std()\n",
    "    var_df = df.var()\n",
    "    \n",
    "    return pd.concat([max_df, min_df, mean_df, std_df, var_df], axis=0)\n",
    "\n",
    "features = {}\n",
    "frames = []\n",
    "\n",
    "for archive in 'xtrain.zip', 'xtest.zip':\n",
    "    with zipfile.ZipFile(archive) as zf:\n",
    "        for name in tqdm_notebook(zf.namelist()):\n",
    "            if name.endswith('.csv'):\n",
    "                \n",
    "                fname = name.split('/')[-1]\n",
    "                Id = int(fname.split('.')[0])\n",
    "\n",
    "                df = pd.read_csv(zf.open(name), header=None)\n",
    "                frames.append(df)\n",
    "                features[Id] = calc_features(df)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features= pd.DataFrame(features).T\n",
    "features.index.name = 'Id'\n",
    "features.reset_index(inplace=True)\n",
    "\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "\n",
    "frames2 = []\n",
    "\n",
    "for archive in 'xtrain.zip', 'xtest.zip':\n",
    "    with zipfile.ZipFile(archive) as zf:\n",
    "        for name in tqdm_notebook(zf.namelist()):\n",
    "            if name.endswith('.csv'):\n",
    "                \n",
    "                fname = name.split('/')[-1]\n",
    "                Id = int(fname.split('.')[0])\n",
    "\n",
    "                df = pd.read_csv(zf.open(name), header=None)\n",
    "                df['Id'] = Id\n",
    "                df['label'] = int(labels['Attack'][Id])\n",
    "                frames2.append(df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Generate features with tsfresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings to try for features generation with tsfresh \n",
    "\n",
    "\n",
    "hack_settings_small = {\n",
    "    'length': None,\n",
    "    'maximum': None,\n",
    "    'mean': None,\n",
    "    'median': None,\n",
    "    'minimum': None,\n",
    "    'standard_deviation': None,\n",
    "    'sum_values': None,\n",
    "    'variance': None,\n",
    "    'abs_energy': None,\n",
    "    'absolute_sum_of_changes': None,\n",
    "    'agg_autocorrelation': [{'f_agg': 'mean'},\n",
    "        {'f_agg': 'median'},\n",
    "        {'f_agg': 'var'}],\n",
    "    'autocorrelation': [\n",
    "        {'lag': 10},\n",
    "        {'lag': 50},\n",
    "        {'lag': 100}],\n",
    "    'c3': [{'lag': 1}, {'lag': 10}],\n",
    "    'cid_ce': [{'normalize': True}, {'normalize': False}],\n",
    "    'count_above_mean': None,\n",
    "    'count_below_mean': None,\n",
    "    'fft_aggregated': [{'aggtype': 'centroid'},\n",
    "        {'aggtype': 'variance'},\n",
    "        {'aggtype': 'skew'},\n",
    "        {'aggtype': 'kurtosis'}],\n",
    "     'fft_coefficient': [{'attr': 'real', 'coeff': 0},\n",
    "        {'attr': 'real', 'coeff': 1},\n",
    "        {'attr': 'real', 'coeff': 2},\n",
    "        {'attr': 'real', 'coeff': 3},\n",
    "        {'attr': 'real', 'coeff': 4},\n",
    "        {'attr': 'real', 'coeff': 5},\n",
    "        ],\n",
    "    'index_mass_quantile': [{'q': 0.1},\n",
    "        {'q': 0.2},\n",
    "        {'q': 0.9}],\n",
    "    'kurtosis': None,\n",
    "    'large_standard_deviation': [\n",
    "        {'r': 0.05},\n",
    "        {'r': 0.9500000000000001}],\n",
    "    'last_location_of_maximum': None,\n",
    "    'last_location_of_minimum': None,\n",
    "    'length': None,\n",
    "    'linear_trend': [\n",
    "        {'attr': 'pvalue'},\n",
    "        {'attr': 'rvalue'},\n",
    "        {'attr': 'intercept'},\n",
    "        {'attr': 'slope'},\n",
    "        {'attr': 'stderr'}],\n",
    "    'mean_abs_change': None,\n",
    "    'mean_change': None,\n",
    "    'mean_second_derivative_central': None,\n",
    "    'partial_autocorrelation': [{'lag': 0},\n",
    "        {'lag': 1},\n",
    "        {'lag': 2},\n",
    "        {'lag': 3},\n",
    "        {'lag': 4},\n",
    "        {'lag': 5},\n",
    "    ],\n",
    "    'quantile': [\n",
    "        {'q': 0.2},\n",
    "        {'q': 0.8}],\n",
    "    'range_count': [{'max': 1, 'min': -1}],\n",
    "    'skewness': None,\n",
    "    'standard_deviation': None,\n",
    "    'variance': None,\n",
    "    'variance_larger_than_standard_deviation': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "i = 1\n",
    "hack_features = []\n",
    "\n",
    "for frame in tqdm(frames2):\n",
    "    ts = frame.copy().drop('label', axis=1)\n",
    "    Id = frame['Id'][0]\n",
    "    ts['time'] = Id\n",
    "    \n",
    "    X = extract_features(ts, \n",
    "                     column_id='Id', column_sort='time',\n",
    "                     default_fc_parameters=hack_settings_small,\n",
    "                     impute_function=impute,\n",
    "                     chunksize=500\n",
    "                    )\n",
    "    \n",
    "#     X.to_csv('./features/{}.csv'.format(Id))\n",
    "    hack_features.append(X)\n",
    "    print(X.shape)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hack_f = pd.concat(hack_features, axis=0)\n",
    "hack_f = hack_f.loc[:, ~hack_f.columns.duplicated()]\n",
    "\n",
    "print(hack_f.shape)\n",
    "print(hack_f.index)\n",
    "\n",
    "hack_f.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering sensors (by correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# collect all ts data in one larde DF \n",
    "\n",
    "frames_for_tall = []\n",
    "for frame in tqdm(frames2):\n",
    "    ts = frame.copy()\n",
    "    Id = frame['Id'][0]\n",
    "    ts['time'] = Id\n",
    "    frames_for_tall.append(ts)\n",
    "    \n",
    "tall_df = pd.concat(frames_for_tall, axis = 0)\n",
    "y = tall_df[['label']]\n",
    "\n",
    "print(tall_df.shape)\n",
    "tall_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = tall_df.copy().drop(['Id', 'label', 'time'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correlation_matrix(df):\n",
    "    from matplotlib import pyplot as plt\n",
    "    from matplotlib import cm as cm\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    cmap = cm.get_cmap('jet', 30)\n",
    "    cax = ax1.imshow(df.corr(), interpolation=\"nearest\", cmap=cmap)\n",
    "    ax1.grid(True)\n",
    "    plt.title('Abalone Feature Correlation')\n",
    "    labels=['Sex','Length','Diam','Height','Whole','Shucked','Viscera','Shell','Rings',]\n",
    "    ax1.set_xticklabels(labels,fontsize=6)\n",
    "    ax1.set_yticklabels(labels,fontsize=6)\n",
    "    # Add colorbar, make sure to specify tick locations to match desired ticklabels\n",
    "    fig.colorbar(cax, ticks=[.75,.8,.85,.90,.95,1])\n",
    "    plt.show()\n",
    "\n",
    "cm = correlation_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "Z=linkage(cm, 'single', 'correlation')\n",
    "dendrogram(Z, color_threshold=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all features dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extracted_features = pd.merge(hack_f.copy().reset_index(), labels, how='left', left_on='id', right_on='Id')\n",
    "extracted_features = extracted_features.T.drop_duplicates().T\n",
    "\n",
    "print(extracted_features.shape)\n",
    "extracted_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[print(col) for col in extracted_features.columns.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved   features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load \n",
    "\n",
    "extracted_features = pd.read_csv('data_features_3251.csv')\n",
    "extracted_features.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load features (from Roman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features = pd.read_csv('./features_Roman.csv')\n",
    "\n",
    "print(extracted_features.shape)\n",
    "extracted_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get trainset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preparing datasets \n",
    "\n",
    "def get_trainset(data, xcols=None, testout=False, test_size=0.2):\n",
    "    \"\"\" \n",
    "    Get train set from features DF\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train = None\n",
    "    y_train = None\n",
    "    X_dev = None\n",
    "    y_dev = None\n",
    "    target = 'Attack'\n",
    "    \n",
    "    if xcols is None:\n",
    "        xcols =  [c for c in data.columns.tolist() if not c in ('Id', 'Attack', 'train')]\n",
    "        \n",
    "    xcols = list(set(xcols))\n",
    "    X_train, y_train = data.loc[data.train == 1, xcols].fillna(0), data.loc[data.train == 1, target]\n",
    "\n",
    "    if testout is True:\n",
    "        X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "        return X_train, X_dev, y_train, y_dev\n",
    "    \n",
    "    return X_train, y_train, X_dev, y_dev\n",
    "\n",
    "def get_testset(data, xcols=None):\n",
    "    \"\"\" \n",
    "    Get train set from features DF\n",
    "    \"\"\"\n",
    "    \n",
    "    X_test = None\n",
    "    y_test = None\n",
    "    target = 'Attack'\n",
    "    \n",
    "    if xcols is None:\n",
    "        xcols =  [c for c in data.columns.tolist() if not c in ('Id', 'Attack', 'train')]\n",
    "        \n",
    "    xcols = list(set(xcols))\n",
    "    X_test, y_test = data.loc[data.train == 0, xcols].fillna(0), data.loc[data.train == 0, target]\n",
    "    \n",
    "    return X_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# target = 'Attack'\n",
    "\n",
    "xtrain, ytrain, _, _ = get_trainset(extracted_features)\n",
    "len(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "RF_params_grid = { \n",
    "    \"n_estimators\" : [60, 100, 150],\n",
    "    \"max_depth\" : [15, 20],\n",
    "    \"min_samples_leaf\" : [1, 2, 4],\n",
    "    'random_state': [42],\n",
    "    'class_weight': ['balanced'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_ExtraTreesClassifier(X_train, y_train, grid_params=None, n_important_f=20, metric='roc_auc'):\n",
    "    \n",
    "    if grid_params is None:\n",
    "        grid_params = { \n",
    "            \"n_estimators\" : [50, 60, 100],\n",
    "            \"max_depth\" : [10, 15],\n",
    "            \"min_samples_leaf\" : [1, 3],\n",
    "            'random_state': [42]\n",
    "        }\n",
    "    \n",
    "    clf = GridSearchCV(estimator=ExtraTreesClassifier(), \n",
    "                          param_grid=grid_params, \n",
    "                          cv=5, verbose=3, n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "    scores = cross_val_score(clf, X_train, y_train, scoring=metric)\n",
    "    print('{}: mean - {}, std - {}'.format(metric, scores.mean(), scores.std()))\n",
    "        \n",
    "    return clf\n",
    "\n",
    "\n",
    "def train_RandomForest(X_train, y_train, grid_params=None, n_important_f=20, metric='roc_auc'):\n",
    "    \n",
    "    if grid_params is None:\n",
    "        grid_params = { \n",
    "            \"n_estimators\" : [50, 60, 100, 150],\n",
    "            \"max_depth\" : [10, 15],\n",
    "            \"min_samples_leaf\" : [1, 3],\n",
    "            'random_state': [42]\n",
    "        }\n",
    "    \n",
    "    clf = GridSearchCV(estimator=RandomForestClassifier(), \n",
    "                          param_grid=grid_params, \n",
    "                          cv=5, verbose=3, n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "    scores = cross_val_score(clf, X_train, y_train, scoring=metric)\n",
    "    print('{}: mean - {}, std - {}'.format(metric, scores.mean(), scores.std()))\n",
    "        \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train RandomForest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RF_clf = train_RandomForest(StandardScaler().fit_transform(xtrain), \n",
    "                            ytrain, grid_params=None, n_important_f=100)\n",
    "\n",
    "RF_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = RF_clf.best_estimator_.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "count = 0\n",
    "imp_f = []\n",
    "\n",
    "for f in range(200):\n",
    "    print('{}) {} - {}'.format(f, xtrain.columns[1:][f], importances[indices[f]]))\n",
    "    imp_f.append(xtrain.columns[1:][f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold, SelectFromModel\n",
    "\n",
    "sm_model = SelectFromModel(RF_clf.best_estimator_, prefit=True)\n",
    "X_small = sm_model.transform(xtrain)\n",
    "print(X_small.shape)\n",
    "\n",
    "X_test_small = sm_model.transform(xtest)\n",
    "print(X_test_small.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add columns name to selected ndarray of features (i.e. functions to train classifiers works with column names)\n",
    "\n",
    "#  select xtrain set\n",
    "X_selected_df = pd.DataFrame(X_small, \n",
    "                             columns=[xtrain.columns[i] for i in range(len(xtrain.columns)) if sm_model.get_support()[i]])\n",
    "\n",
    "\n",
    "print(X_selected_df.shape)\n",
    "X_selected_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  select xtest set\n",
    "\n",
    "X_test_selected_df = pd.DataFrame(X_test_small, \n",
    "                             columns=[xtest.columns[i] for i in range(len(xtest.columns)) if sm_model.get_support()[i]])\n",
    "\n",
    "print(X_test_selected_df.shape)\n",
    "X_test_selected_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run RF train again with selected features \n",
    "\n",
    "RF_clf_small = train_RandomForest(StandardScaler().fit_transform(X_small), \n",
    "                                  ytrain, grid_params=None, n_important_f=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExT_clf_small = train_ExtraTreesClassifier(StandardScaler().fit_transform(X_small), \n",
    "                                     ytrain, grid_params=None, n_important_f=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=55)\n",
    "X_pca = pca.fit_transform(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RF_clf_pca = train_RandomForest(StandardScaler().fit_transform(X_pca), \n",
    "                                ytrain, grid_params=None, n_important_f=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastICA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "ica = FastICA(n_components=100)\n",
    "X_ica = ica.fit_transform(xtrain)  # Reconstruct signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RF_clf_ica = train_RandomForest(X_ica, ytrain, grid_params=None, n_important_f=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### GradientBoostingClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_grid_params = {\n",
    "    'learning_rate': [0.1, 0.01],\n",
    "    'max_depth': [4, 16],\n",
    "    'min_samples_leaf': [1, 3],\n",
    "    \"n_estimators\" : [30, 60, 100],\n",
    "    #'max_features': [1.0, 0.3, 0.1] \n",
    "    }\n",
    "\n",
    "clf_GB = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# trained with initial features set\n",
    "clf_GB = GridSearchCV(estimator=clf_GB, \n",
    "                      param_grid=gb_grid_params, \n",
    "                      cv=5, verbose=3, n_jobs=-1).fit(xtrain, ytrain)\n",
    "\n",
    "scores = cross_val_score(clf_GB, xtrain, ytrain, scoring='roc_auc')\n",
    "scores.mean(), scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# selected features only \n",
    "clf_GB = GridSearchCV(estimator=clf_GB, \n",
    "                      param_grid=gb_grid_params, \n",
    "                      cv=5, verbose=3, n_jobs=-1).fit(X_small, ytrain)\n",
    "\n",
    "scores = cross_val_score(clf_GB, X_small, ytrain, scoring='roc_auc')\n",
    "scores.mean(), scores.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  VotingClassifier with GridSearch (this gave score = .95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "estimators = [('lr', clf1), ('rf', clf2), ('gnb', clf3)]\n",
    "\n",
    "eclf = VotingClassifier(estimators=estimators, voting='soft')\n",
    "\n",
    "params = {'lr__C': [0.01, 0.1, 1.0, 10.0], \n",
    "          'lr__penalty': ['l1'],\n",
    "          'rf__n_estimators': [60, 100, 150],\n",
    "          \"rf__max_depth\" : [6, 10, 15],\n",
    "          \"rf__min_samples_leaf\" : [1, 3],\n",
    "          'rf__random_state': [42]\n",
    "         }\n",
    "\n",
    "vote_clf = GridSearchCV(estimator=eclf, \n",
    "                    param_grid=params, \n",
    "                    cv=5, verbose=3, n_jobs=-1).fit(X_small, ytrain)\n",
    "                   \n",
    "scores = cross_val_score(vote_clf, \n",
    "                         StandardScaler().fit_transform(X_small), \n",
    "                         ytrain, scoring='roc_auc')\n",
    "\n",
    "print('VOTING RESULTS: ', scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vote_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "bc_params = {\"base_estimator__max_depth\": [15],\n",
    "          \"base_estimator__max_features\": [None, \"auto\"],\n",
    "          \"base_estimator__min_samples_leaf\": [1, 3],\n",
    "          \"base_estimator__min_samples_split\": [2, 5],\n",
    "          'bootstrap_features': [False, True],\n",
    "          'max_features': [0.2, 0.3],\n",
    "          'max_samples': [0.5, 1.0],\n",
    "          'n_estimators': [60, 150],\n",
    "}\n",
    "\n",
    "bag_clf = GridSearchCV(estimator=BaggingClassifier(RandomForestClassifier()), \n",
    "                       param_grid=bc_params, \n",
    "                       cv=5, verbose=3, n_jobs=-1).fit(xtrain, ytrain)\n",
    "\n",
    "scores = cross_val_score(bag_clf, \n",
    "                         StandardScaler().fit_transform(X_small), \n",
    "                         ytrain, scoring='roc_auc')\n",
    "print('BAGGING RESULTS: ', scores.mean(), scores.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembles of Classifiers that Operate on Different Feature Subsets\n",
    "\n",
    "https://rasbt.github.io/mlxtend/user_guide/classifier/EnsembleVoteClassifier/#example-6-ensembles-of-classifiers-that-operate-on-different-feature-subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "columns = xtrain.columns.tolist()\n",
    "\n",
    "col1= columns[0:1000]\n",
    "col2= columns[0:1000]\n",
    "          \n",
    "\n",
    "\n",
    "pipe1 = make_pipeline(ColumnSelector(cols=range(0, 100)),\n",
    "                      LogisticRegression())\n",
    "pipe2 = make_pipeline(ColumnSelector(cols=range(100, 200)),\n",
    "                      LogisticRegression())\n",
    "\n",
    "evclf = EnsembleVoteClassifier(clfs=[pipe1, pipe2])\n",
    "\n",
    "scores = cross_val_score(evclf, xtrain, ytrain, scoring='roc_auc')\n",
    "print('RESULTS: ', scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Custom crazy ensemble  (gave score = .92 somehow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try to get custome ensembe (this gave 0.92 scores)\n",
    "\n",
    "def get_features_for_sensor(df, sensors):\n",
    "    \"\"\"\n",
    "    Extract features for the cluster of sensors\n",
    "    \"\"\"\n",
    "     \n",
    "    columns = df.columns.tolist()\n",
    "    sensor_features = []\n",
    "    \n",
    "    for sensor in sensors:\n",
    "        for col in columns:\n",
    "            name = str(col)\n",
    "            if name.endswith('_{}'.format(str(sensor))):\n",
    "                sensor_features.append(col)\n",
    "    \n",
    "    return df.copy()[sensor_features]\n",
    "\n",
    "\n",
    "def train_ExT_cluster(X, sensors, n, metric):\n",
    "    \"\"\"\n",
    "    Train classifier for a cluster of sensors\n",
    "    \n",
    "    n: number of features to select for a model\n",
    "    \"\"\"\n",
    "    df = get_features_for_sensor(X, sensors)\n",
    "    print(df.shape)\n",
    "    \n",
    "    pca = PCA(n_components=n).fit(df)\n",
    "    X_in = StandardScaler().fit_transform(pca.transform(df))\n",
    "    print(X_in.shape)\n",
    "\n",
    "    clf = train_ExtraTreesClassifier(X_in, ytrain, grid_params=None, n_important_f=20, metric=metric\n",
    "    \n",
    "    return clf, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clusters of sensors (based on features clustering)\n",
    "\n",
    "c1 = [6, 10, 23, 29, 38, 47]\n",
    "c2 = [3, 5, 22, 27, 28, 33, 34, 35,36, 41, 43]\n",
    "c3 = [1, 4, 17, 44, 45, 49, 54]\n",
    "c4 = [5, 20, 25, 31, 50]\n",
    "c5 = [2, 3, 13, 24, 26, 31, 32, 33, 34, 40, 42, 46]\n",
    "c6 = [0, 7, 9, 11, 12, 15, 17, 21, 35, 53]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train classifier for each cluster \n",
    "\n",
    "c1_ExT_clf, c1_pca = train_ExT_cluster(X_selected_df, c1, 28, 'precision')\n",
    "c2_ExT_clf, c2_pca = train_ExT_cluster(X_selected_df, c2, 28, 'precision')\n",
    "c3_ExT_clf, c3_pca = train_ExT_cluster(X_selected_df, c3, 28, 'precision')\n",
    "c4_ExT_clf, c4_pca = train_ExT_cluster(X_selected_df, c4, 28, 'precision')\n",
    "c5_ExT_clf, c5_pca = train_ExT_cluster(X_selected_df, c5, 28, 'precision')\n",
    "c6_ExT_clf, c6_pca = train_ExT_cluster(X_selected_df, c6, 28, 'precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge cluster classifiers into an Ensemble (as a dict) \n",
    "\n",
    "c_ensemble_clf = {\n",
    "    'c1': {\n",
    "        'sensors': c1,\n",
    "        'clf': c1_ExT_clf,\n",
    "        'pca':  c1_pca\n",
    "    },\n",
    "    'c2': {\n",
    "        'sensors': c2,\n",
    "        'clf': c2_ExT_clf,\n",
    "        'pca':  c2_pca\n",
    "    },\n",
    "    'c3': {\n",
    "        'sensors': c3,\n",
    "        'clf': c3_ExT_clf,\n",
    "        'pca':  c3_pca\n",
    "    },\n",
    "    'c4':{\n",
    "        'sensors': c4,\n",
    "        'clf': c4_ExT_clf,\n",
    "        'pca':  c4_pca\n",
    "    },\n",
    "    'c5': {\n",
    "        'sensors': c5,\n",
    "        'clf': c5_ExT_clf,\n",
    "        'pca':  c5_pca\n",
    "    },\n",
    "    'c6':{\n",
    "        'sensors': c6,\n",
    "        'clf': c6_ExT_clf,\n",
    "        'pca':  c6_pca\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def custom_ensemble_clf(data, c_ensemble_clf):\n",
    "    \"\"\"\n",
    "    Get prediction for data, get max scores over clusters individual predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    for member in c_ensemble_clf.keys():\n",
    "        print(member)\n",
    "        \n",
    "        clf = c_ensemble_clf[member]['clf']\n",
    "        sensors = c_ensemble_clf[member]['sensors']\n",
    "        pca = c_ensemble_clf[member]['pca']\n",
    "        \n",
    "        c_data = pca.transform(get_features_for_sensor(data, sensors))\n",
    "        print(c_data.shape)\n",
    "        results.append(clf.predict_proba(c_data))\n",
    "    \n",
    "    return np.max( np.array(results), axis=0 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = custom_ensemble_clf(X_test_selected_df, c_ensemble_clf)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get prediction for models, train with all features \n",
    "\n",
    "# clf = vote_clf\n",
    "\n",
    "# data = extracted_features\n",
    "# xcols =  [c for c in data.columns.tolist() if not c in ('Id', 'Attack', 'train')]\n",
    "\n",
    "# xtest,  ytest = get_testset(extracted_features)\n",
    "# print(xtest.shape, ytest.shape)\n",
    "\n",
    "# clf.fit(xtrain, ytrain)\n",
    "# prediction = clf.predict_proba(xtest)[:,1]\n",
    "\n",
    "# sample_submission = pd.read_csv('SampleSubmission.csv')\n",
    "# sample_submission['Attack'] = prediction\n",
    "# sample_submission.to_csv('baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get predicitons for custome ensemble \n",
    "\n",
    "prediction = custom_ensemble_clf(X_test_selected_df, c_ensemble_clf)[:,1]\n",
    "\n",
    "sample_submission = pd.read_csv('SampleSubmission.csv')\n",
    "sample_submission['Attack'] = prediction\n",
    "sample_submission.to_csv('baseline_v5_CrazyCustom.csv', index=False)\n",
    "\n",
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
